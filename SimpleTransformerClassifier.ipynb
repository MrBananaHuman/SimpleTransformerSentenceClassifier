{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleTransformerClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qBke5tuwsC-",
        "colab_type": "code",
        "outputId": "2a32c825-4ba7-4b2b-8fe4-6420c23edd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WG9bY3twtJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "063d4b9c-cf16-45a5-965a-837323303b05"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RaIg1JxnwI",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2tqEDCXxZSn",
        "colab_type": "code",
        "outputId": "cc452a77-3964-4f74-8461-0c597f0c147d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data = open('/content/drive/My Drive/transformer_classification/train_data.txt')\n",
        "\n",
        "lines = data.readlines()\n",
        "sent_data = []\n",
        "label_data = []\n",
        "\n",
        "total_char = set()\n",
        "total_label = set()\n",
        "total_data_num = 0\n",
        "\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    sent_data.append(line.split('\\t')[1])\n",
        "    label_data.append(int(line.split('\\t')[0]))\n",
        "    total_label.add(int(line.split('\\t')[0]))\n",
        "    total_data_num += 1\n",
        "    for chars in line.split('\\t')[1]:\n",
        "        total_char.add(chars)\n",
        "\n",
        "print('total char num: ' + str(len(total_char)))\n",
        "print('total data num: ' + str(total_data_num))\n",
        "\n",
        "data.close()\n",
        "\n",
        "# label_num = len(total_label)\n",
        "label_num = 508\n",
        "print('total label num: ' + str(label_num))\n",
        "vocabs = tfds.features.text.SubwordTextEncoder.build_from_corpus((sent for sent in sent_data), target_vocab_size=1600)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total char num: 1254\n",
            "total data num: 1001\n",
            "total label num: 508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axm-wD39za5P",
        "colab_type": "code",
        "outputId": "57aee7e5-2cc5-4e40-9e82-7353bcd19960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(vocabs.subwords[0:10])\n",
        "print(vocabs.vocab_size)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['・', 'の', 'ル', 'ス', '日本の', 'ン', 'ー', 'イ', 'の選', 'ト']\n",
            "1659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWHhgRGycEU",
        "colab_type": "code",
        "outputId": "63355282-dd05-4436-b1ee-b0d8ae9d250f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "print(sent_data[0])\n",
        "tokenized_string = vocabs.encode(sent_data[0])\n",
        "for ts in tokenized_string:\n",
        "    print ('{}: {}'.format(vocabs.decode([ts]), ts))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "鹿児島県の鉄道日本の鉄道駅おおすみおおかわ九州旅客鉄道の鉄道日豊本線おおすみおおかわらえ日本国有鉄道の鉄道1931年開業の鉄道曽於市の交通おおすみおおかわらえ曽於市の建築\n",
            "�: 1636\n",
            "�: 1588\n",
            "�: 1594\n",
            "�: 1632\n",
            "�: 1536\n",
            "�: 1547\n",
            "島県の: 1177\n",
            "鉄道日本の鉄道駅: 1106\n",
            "おお: 227\n",
            "すみ: 1063\n",
            "おお: 227\n",
            "かわ: 650\n",
            "九州旅客鉄道の鉄道: 1210\n",
            "日: 76\n",
            "豊: 1117\n",
            "本線: 1150\n",
            "おお: 227\n",
            "すみ: 1063\n",
            "おお: 227\n",
            "かわ: 650\n",
            "ら: 140\n",
            "え: 21\n",
            "日本国有鉄道の鉄道: 280\n",
            "193: 359\n",
            "1年開業の鉄道: 1390\n",
            "�: 1633\n",
            "�: 1558\n",
            "�: 1592\n",
            "�: 1633\n",
            "�: 1553\n",
            "�: 1591\n",
            "市の交通: 305\n",
            "おお: 227\n",
            "すみ: 1063\n",
            "おお: 227\n",
            "かわ: 650\n",
            "ら: 140\n",
            "え: 21\n",
            "�: 1633\n",
            "�: 1558\n",
            "�: 1592\n",
            "�: 1633\n",
            "�: 1553\n",
            "�: 1591\n",
            "市の建築: 685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XuDS2_-zzA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make one hot vectors\n",
        "# label_one_hot_vector =  np.zeros((total_data_num, label_num))\n",
        "# for i, data in enumerate(label_data):\n",
        "#     label_one_hot_vector[i, data] = 1\n",
        "\n",
        "# print(label_one_hot_vector[0])\n",
        "\n",
        "# lines_dataset = (\n",
        "#     tf.data.Dataset.from_tensor_slices(\n",
        "#         (\n",
        "#             tf.cast(np.array(sent_data), tf.string),\n",
        "#             tf.cast(np.array(label_one_hot_vector), tf.float32)           \n",
        "#         )\n",
        "#     )\n",
        "# )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-brISmpydUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(np.array(sent_data), tf.string),\n",
        "            tf.cast(np.array(label_data), tf.float32)           \n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvJ1BrqynCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmDCcy_Q_yfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(sent, label):\n",
        "    sent = [vocabs.vocab_size] + vocabs.encode(sent.numpy()) + [vocabs.vocab_size+1]\n",
        "    # return sent, label\n",
        "    return sent, [label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NARbrlPQ6MnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "    return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boXb1hIB6OTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(sent,label):\n",
        "    return tf.py_function(encode, [sent,label], [tf.int64, tf.float32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBWE80KT6Sos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = lines_dataset.map(tf_encode)\n",
        "# train_dataset = train_dataset.filter(filter_max_length)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qehmIXWb99",
        "colab_type": "code",
        "outputId": "c1afe056-3a83-42c3-b869-44cbdb4ea4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((?, ?), (?, ?)), types: (tf.int64, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZraJiIHqV_NF",
        "colab_type": "code",
        "outputId": "c2ac3c31-99c6-42c7-f88d-d274b80055f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sent_batch, label_batch = next(iter(train_dataset))\n",
        "sent_batch, label_batch"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=212049, shape=(64, 104), dtype=int64, numpy=\n",
              " array([[1659,  151,  773, ...,    0,    0,    0],\n",
              "        [1659,  271,   80, ...,    0,    0,    0],\n",
              "        [1659,  118,  228, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [1659,  869,    6, ...,    0,    0,    0],\n",
              "        [1659,  470,    2, ...,    0,    0,    0],\n",
              "        [1659, 1126, 1047, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: id=212050, shape=(64, 1), dtype=float32, numpy=\n",
              " array([[337.],\n",
              "        [361.],\n",
              "        [476.],\n",
              "        [418.],\n",
              "        [418.],\n",
              "        [148.],\n",
              "        [ 60.],\n",
              "        [337.],\n",
              "        [ 35.],\n",
              "        [233.],\n",
              "        [133.],\n",
              "        [200.],\n",
              "        [ 53.],\n",
              "        [418.],\n",
              "        [ 99.],\n",
              "        [153.],\n",
              "        [ 80.],\n",
              "        [144.],\n",
              "        [  0.],\n",
              "        [200.],\n",
              "        [300.],\n",
              "        [162.],\n",
              "        [222.],\n",
              "        [275.],\n",
              "        [ 81.],\n",
              "        [267.],\n",
              "        [170.],\n",
              "        [263.],\n",
              "        [265.],\n",
              "        [418.],\n",
              "        [425.],\n",
              "        [153.],\n",
              "        [370.],\n",
              "        [222.],\n",
              "        [234.],\n",
              "        [283.],\n",
              "        [145.],\n",
              "        [352.],\n",
              "        [377.],\n",
              "        [ 49.],\n",
              "        [  5.],\n",
              "        [410.],\n",
              "        [418.],\n",
              "        [476.],\n",
              "        [418.],\n",
              "        [ 85.],\n",
              "        [ 39.],\n",
              "        [ 36.],\n",
              "        [219.],\n",
              "        [295.],\n",
              "        [127.],\n",
              "        [ 15.],\n",
              "        [433.],\n",
              "        [338.],\n",
              "        [378.],\n",
              "        [413.],\n",
              "        [337.],\n",
              "        [400.],\n",
              "        [266.],\n",
              "        [117.],\n",
              "        [250.],\n",
              "        [314.],\n",
              "        [200.],\n",
              "        [337.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugVbKeLt-lSw",
        "colab_type": "text"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZRLWvq0wLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                              np.arange(d_model)[np.newaxis, :],\n",
        "                              d_model)\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "    cosines = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPNQ3NYPAMxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hk1LqK5AP8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDbhb7LlATi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWplyNnAW9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "    \n",
        "        assert d_model % self.num_heads == 0\n",
        "    \n",
        "        self.depth = d_model // self.num_heads\n",
        "    \n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "    \n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "    \n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "    \n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "    \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xytbpFwAYI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8dZKdCAcHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "    \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "        return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbMkLAayAiad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x) \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        \n",
        "        return x \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlHugaoHAoRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                   target_vocab_size, label_num, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, rate)\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(d_model, activation='tanh')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(label_num, activation='softmax')\n",
        "        \n",
        "\n",
        "    def call(self, inp, training, enc_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        enc_output = self.dense1(enc_output[:,0])\n",
        "        enc_output = self.dropout1(enc_output, training=training)\n",
        "        final_output = self.final_layer(enc_output)\n",
        "\n",
        "        return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUXPf2j2As6r",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4r8FOntArlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 6\n",
        "d_model = 256\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = vocabs.vocab_size + 2\n",
        "target_vocab_size = vocabs.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8pzSwLXA0xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "    \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhEzjjtA50C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOnoIMYjA7DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FrYZshcA-iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5zyAIZiBAU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVhNzG3cmbFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, label_num, dropout_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xg5qgROQOMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    return enc_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9nAM4CBEic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/transformer_classification/model_output\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint is restored')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcuwliATMDtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SGZlvECMTpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    enc_padding_mask = create_masks(inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(inp, True, enc_padding_mask)\n",
        "        loss = loss_function(tar, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY467JFTQsHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> imdb comments, tar -> polarity of the comments\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                             ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Train_Loss {:.4f} Train_Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                    train_loss.result(), \n",
        "                                                    train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
