{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleTransformerClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qBke5tuwsC-",
        "colab_type": "code",
        "outputId": "319f67d3-e680-4124-e99b-349f280f20c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WG9bY3twtJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "4c84aa8c-3184-439b-9483-810c4b226a1f"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RaIg1JxnwI",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2tqEDCXxZSn",
        "colab_type": "code",
        "outputId": "393806ca-e636-44e3-e4b8-ba01298f3d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data = open('/content/drive/My Drive/transformer_classification/train_data.txt')\n",
        "# data = open('/content/drive/My Drive/nsmc/ratings_train_small.txt')\n",
        "\n",
        "lines = data.readlines()\n",
        "sent_data = []\n",
        "label_data = []\n",
        "\n",
        "total_char = set()\n",
        "total_label = set()\n",
        "total_data_num = 0\n",
        "\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    sent_data.append(line.split('\\t')[1])\n",
        "    label_data.append(int(line.split('\\t')[0]))\n",
        "    total_label.add(int(line.split('\\t')[0]))\n",
        "    total_data_num += 1\n",
        "    for chars in line.split('\\t')[1]:\n",
        "        total_char.add(chars)\n",
        "\n",
        "print('total char num: ' + str(len(total_char)))\n",
        "print('total data num: ' + str(total_data_num))\n",
        "\n",
        "data.close()\n",
        "\n",
        "label_num = len(total_label)\n",
        "# label_num = 2\n",
        "print('total label num: ' + str(label_num))\n",
        "#vocabs = tfds.features.text.SubwordTextEncoder.build_from_corpus((sent for sent in sent_data), target_vocab_size=2800)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total char num: 2762\n",
            "total data num: 93027\n",
            "total label num: 508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSyp-VYI9YKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the vocabs\n",
        "#vocabs.save_to_file('/content/drive/My Drive/transformer_classification/vocab')\n",
        "\n",
        "# Load\n",
        " vocabs = tfds.features.text.SubwordTextEncoder.load_from_file('/content/drive/My Drive/transformer_classification/vocab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axm-wD39za5P",
        "colab_type": "code",
        "outputId": "60eeebe7-2eb6-4d20-e2ef-9347560e6c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(vocabs.subwords[0:10])\n",
        "print(vocabs.vocab_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['・', 'の', '日本の', 'アメリカ合衆国の', 'ス', 'ル', 'の選', 'ン', 'ー', 'ト']\n",
            "2804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWHhgRGycEU",
        "colab_type": "code",
        "outputId": "76fa4435-b729-4a5e-c233-6c4ca4dc3713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "print(sent_data[0])\n",
        "tokenized_string = vocabs.encode(sent_data[0])\n",
        "for ts in tokenized_string:\n",
        "    print ('{}: {}'.format(vocabs.decode([ts]), ts))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "北部州スリランカスリランカの\n",
            "北: 111\n",
            "部: 195\n",
            "州: 75\n",
            "スリ: 2168\n",
            "ラン: 199\n",
            "カス: 1249\n",
            "リラ: 2435\n",
            "ン: 8\n",
            "カの: 1021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XuDS2_-zzA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make one hot vectors\n",
        "# label_one_hot_vector =  np.zeros((total_data_num, label_num))\n",
        "# for i, data in enumerate(label_data):\n",
        "#     label_one_hot_vector[i, data] = 1\n",
        "\n",
        "# print(label_one_hot_vector[0])\n",
        "\n",
        "# lines_dataset = (\n",
        "#     tf.data.Dataset.from_tensor_slices(\n",
        "#         (\n",
        "#             tf.cast(np.array(sent_data), tf.string),\n",
        "#             tf.cast(np.array(label_one_hot_vector), tf.float32)           \n",
        "#         )\n",
        "#     )\n",
        "# )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-brISmpydUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(np.array(sent_data), tf.string),\n",
        "            tf.cast(np.array(label_data), tf.float32)           \n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvJ1BrqynCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmDCcy_Q_yfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(sent, label):\n",
        "    sent = [vocabs.vocab_size] + vocabs.encode(sent.numpy()) + [vocabs.vocab_size+1]\n",
        "    # return sent, label\n",
        "    return sent, [label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NARbrlPQ6MnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "    return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boXb1hIB6OTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(sent,label):\n",
        "    return tf.py_function(encode, [sent,label], [tf.int64, tf.float32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBWE80KT6Sos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = lines_dataset.map(tf_encode)\n",
        "# train_dataset = train_dataset.filter(filter_max_length)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qehmIXWb99",
        "colab_type": "code",
        "outputId": "00adc62a-e4dd-4fc9-9f9b-b7d1b2bedce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((?, ?), (?, ?)), types: (tf.int64, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZraJiIHqV_NF",
        "colab_type": "code",
        "outputId": "39a0eec9-aecd-422d-cd31-c1145cf0a0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sent_batch, label_batch = next(iter(train_dataset))\n",
        "sent_batch, label_batch"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=100349, shape=(64, 146), dtype=int64, numpy=\n",
              " array([[2804,    3,  324, ...,    0,    0,    0],\n",
              "        [2804,    3,  918, ...,    0,    0,    0],\n",
              "        [2804,  497, 2805, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [2804,  435, 1053, ...,    0,    0,    0],\n",
              "        [2804,  951,  471, ..., 2504,   24, 2805],\n",
              "        [2804,  550,  391, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: id=100350, shape=(64, 1), dtype=float32, numpy=\n",
              " array([[222.],\n",
              "        [232.],\n",
              "        [480.],\n",
              "        [418.],\n",
              "        [265.],\n",
              "        [337.],\n",
              "        [216.],\n",
              "        [418.],\n",
              "        [461.],\n",
              "        [ 93.],\n",
              "        [408.],\n",
              "        [494.],\n",
              "        [492.],\n",
              "        [337.],\n",
              "        [439.],\n",
              "        [470.],\n",
              "        [243.],\n",
              "        [  5.],\n",
              "        [186.],\n",
              "        [288.],\n",
              "        [221.],\n",
              "        [211.],\n",
              "        [223.],\n",
              "        [200.],\n",
              "        [429.],\n",
              "        [ 39.],\n",
              "        [275.],\n",
              "        [326.],\n",
              "        [ 36.],\n",
              "        [337.],\n",
              "        [242.],\n",
              "        [222.],\n",
              "        [342.],\n",
              "        [222.],\n",
              "        [ 74.],\n",
              "        [272.],\n",
              "        [314.],\n",
              "        [361.],\n",
              "        [222.],\n",
              "        [328.],\n",
              "        [492.],\n",
              "        [104.],\n",
              "        [295.],\n",
              "        [423.],\n",
              "        [291.],\n",
              "        [402.],\n",
              "        [307.],\n",
              "        [361.],\n",
              "        [173.],\n",
              "        [153.],\n",
              "        [  5.],\n",
              "        [216.],\n",
              "        [396.],\n",
              "        [337.],\n",
              "        [337.],\n",
              "        [ 22.],\n",
              "        [429.],\n",
              "        [ 30.],\n",
              "        [ 99.],\n",
              "        [ 80.],\n",
              "        [153.],\n",
              "        [ 81.],\n",
              "        [216.],\n",
              "        [418.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugVbKeLt-lSw",
        "colab_type": "text"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZRLWvq0wLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                              np.arange(d_model)[np.newaxis, :],\n",
        "                              d_model)\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "    cosines = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPNQ3NYPAMxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hk1LqK5AP8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDbhb7LlATi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWplyNnAW9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "    \n",
        "        assert d_model % self.num_heads == 0\n",
        "    \n",
        "        self.depth = d_model // self.num_heads\n",
        "    \n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "    \n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "    \n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "    \n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "    \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xytbpFwAYI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8dZKdCAcHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "    \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "        return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbMkLAayAiad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x) \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        \n",
        "        return x \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlHugaoHAoRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                   target_vocab_size, label_num, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, rate)\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(d_model, activation='tanh')\n",
        "        # self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(label_num, activation='softmax')\n",
        "        \n",
        "\n",
        "    def call(self, inp, training, enc_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        enc_output = self.dense1(enc_output[:,0])\n",
        "        # enc_output = self.dropout1(enc_output, training=training)\n",
        "        final_output = self.final_layer(enc_output)\n",
        "\n",
        "        return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUXPf2j2As6r",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4r8FOntArlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 1\n",
        "d_model = 256\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = vocabs.vocab_size + 2\n",
        "target_vocab_size = vocabs.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8pzSwLXA0xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "    \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhEzjjtA50C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "#learning_rate = 0.0003\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOnoIMYjA7DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FrYZshcA-iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5zyAIZiBAU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVhNzG3cmbFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, label_num, dropout_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xg5qgROQOMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    return enc_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9nAM4CBEic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/transformer_classification/model_output\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint is restored')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcuwliATMDtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SGZlvECMTpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    enc_padding_mask = create_masks(inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(inp, True, enc_padding_mask)\n",
        "        loss = loss_function(tar, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY467JFTQsHd",
        "colab_type": "code",
        "outputId": "8d28431e-3e60-4e7b-c767-a4fd9b93c6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> imdb comments, tar -> polarity of the comments\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                             ckpt_save_path))\n",
        "        print ('Epoch {} Train_Loss {:.4f} Train_Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                    train_loss.result(), \n",
        "                                                    train_accuracy.result()))\n",
        "        print ('Time taken for 5 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 6.1042 Accuracy 0.1250\n",
            "Epoch 1 Batch 100 Loss 6.0819 Accuracy 0.1491\n",
            "Epoch 1 Batch 200 Loss 6.0640 Accuracy 0.1636\n",
            "Epoch 1 Batch 300 Loss 6.0516 Accuracy 0.1769\n",
            "Epoch 1 Batch 400 Loss 6.0457 Accuracy 0.1818\n",
            "Epoch 1 Batch 500 Loss 6.0388 Accuracy 0.1887\n",
            "Epoch 1 Batch 600 Loss 6.0311 Accuracy 0.1963\n",
            "Epoch 1 Batch 700 Loss 6.0263 Accuracy 0.2013\n",
            "Epoch 1 Batch 800 Loss 6.0194 Accuracy 0.2078\n",
            "Epoch 1 Batch 900 Loss 6.0139 Accuracy 0.2133\n",
            "Epoch 1 Batch 1000 Loss 6.0069 Accuracy 0.2203\n",
            "Epoch 1 Batch 1100 Loss 6.0017 Accuracy 0.2255\n",
            "Epoch 1 Batch 1200 Loss 5.9969 Accuracy 0.2301\n",
            "Epoch 1 Batch 1300 Loss 5.9926 Accuracy 0.2345\n",
            "Epoch 1 Batch 1400 Loss 5.9871 Accuracy 0.2397\n",
            "Epoch 2 Batch 0 Loss 5.9560 Accuracy 0.2812\n",
            "Epoch 2 Batch 100 Loss 5.9037 Accuracy 0.3275\n",
            "Epoch 2 Batch 200 Loss 5.8999 Accuracy 0.3274\n",
            "Epoch 2 Batch 300 Loss 5.8988 Accuracy 0.3296\n",
            "Epoch 2 Batch 400 Loss 5.8997 Accuracy 0.3281\n",
            "Epoch 2 Batch 500 Loss 5.8968 Accuracy 0.3310\n",
            "Epoch 2 Batch 600 Loss 5.8938 Accuracy 0.3338\n",
            "Epoch 2 Batch 700 Loss 5.8927 Accuracy 0.3352\n",
            "Epoch 2 Batch 800 Loss 5.8894 Accuracy 0.3382\n",
            "Epoch 2 Batch 900 Loss 5.8873 Accuracy 0.3402\n",
            "Epoch 2 Batch 1000 Loss 5.8839 Accuracy 0.3436\n",
            "Epoch 2 Batch 1100 Loss 5.8816 Accuracy 0.3458\n",
            "Epoch 2 Batch 1200 Loss 5.8793 Accuracy 0.3479\n",
            "Epoch 2 Batch 1300 Loss 5.8779 Accuracy 0.3495\n",
            "Epoch 2 Batch 1400 Loss 5.8753 Accuracy 0.3519\n",
            "Epoch 3 Batch 0 Loss 5.9036 Accuracy 0.3281\n",
            "Epoch 3 Batch 100 Loss 5.8412 Accuracy 0.3895\n",
            "Epoch 3 Batch 200 Loss 5.8402 Accuracy 0.3874\n",
            "Epoch 3 Batch 300 Loss 5.8412 Accuracy 0.3871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMdMNtT92qfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}